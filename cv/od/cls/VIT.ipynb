{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 基础概念\n",
    "\n",
    "在nlp里面，cls是对于句子首位置的一个特殊token,在bert里用来做句子的分类信息，在此也用作图像的分类信息。\n",
    "![img](../pic/VIT.png)  \n",
    "  \n",
    "VIT本质是给输入的图像块做encoder  \n",
    "  \n",
    "  <center>\n",
    "  \n",
    "![img](../pic/VIT2.png)\n",
    "  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 整体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "    \n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool='cls', channels=3,\n",
    "                 dim_head=64, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)  # 224*224\n",
    "        patch_height, patch_width = pair(patch_size)  # 16 * 16\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)  \n",
    "        # 224//16 **2 =14*14=196\n",
    "        patch_dim = channels * patch_height * patch_width  \n",
    "        # patch的w乘h乘通道数c,将三维16*16*3=768拉平到二维\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
    "            # (h p1)是将原本的224切分成了(224//16=14, 16);h,w是指高宽方向patch的个数。\n",
    "            # 从(1,3,224,224)-->(1,196,768);;224的图片可分14*14个16尺寸的patch，\n",
    "            # 一张图片共切14*14=196个patch。768=16*16*3\n",
    "            nn.Linear(patch_dim, dim)  # 从768--1024\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))  \n",
    "        # 生成位置编码，cls和token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))  # 初始化cls\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)  # img (1, 3, 224, 224)  输出形状x : (1, 196, 1024)\n",
    "        # patch形状为16*16，拉平为3*16*16=768，再通过Linear转为dim为1024（人为设置dim，是emb维度）则每个patch图像块对应的维度为1024.\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)  \n",
    "        # 复制batch份，每个batch都要加一个cls---(bs,1,dim)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)  # (1,197,1024) 每张图就一个cls\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)  # 输入输出维度不变！都是(1,197,1024)\n",
    "\n",
    "        x = x.mean(dim=1) if self.pool == 'mean' else x[:, 0]  # 提取第一个cls\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n",
    "        \n",
    "v = ViT(\n",
    "    image_size=224,  # 输入图像wh\n",
    "    patch_size=16,\n",
    "    num_classes=1000,\n",
    "    dim=1024,  # dim是emb维度\n",
    "    depth=6,  # encorder的个数\n",
    "    heads=16,  # 多头注意力有几个头\n",
    "    mlp_dim=2048,\n",
    "    dropout=0.1,\n",
    "    emb_dropout=0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "preds = v(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 明确一点，在送完emb+pos进去之后，输出的形状没有变，送进去是(1,197,1024),输出也是(1,197,1024),可以用来接不同任务的下游。\n",
    "- 由于原vit是分类任务，所以cls的操作是沿用bert的，每个输出前面设置一个cls来做整个图片的特征，在这里对所有的做mean(1)也可以。得到的都是(1,1024)  \n",
    "  \n",
    "**描述概括黑箱:**\n",
    "- 对输入的图片先Linear扩增三分kqv，再经过nheads头的dotattention，最终再将nheads的输出拼接起来做Linear，完成MHA后(与上面的MHA一摸一样),再接一个MLP(输入输出维度不变)。\n",
    "- kqv的形状为(bs,heads,n,emb_dim/heads),其中n为一张图片切出的块数(再加cls),进行完QK^T之后得到的是(bs,heads,n,n),这其中就表示了n个块之间的全局关系，在经过一个softmax后与V进行matmul即得到out。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2拆黑箱  \n",
    "首先看Transformer整体，首先对每一次输入做一个LayerNorm，输入一个self-Attention和一个mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "        \n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 注意，这里送进去attention的只有一个x，哪怕是nlp里面，kqv也是要送进去三个的(见书305页，或上面的MHA例子送进去的XYY)，所以这里要先将单个的x形成三分kqv，再送进去，这与nheads的观念并无冲突！！！\n",
    "2. 送进去后，利用chunk分块，再使用map这行命令，将(bs,n,1024)分成了(bs,heads,n,1024/heads),这与上面的MHA也对上了！！！\n",
    "3. 经过上面的操作，kqv都是(bs,heads,n,1024/heads),此时进行attention操作，注意这里的torch.matmul只对后两维进行矩阵乘法，进行完softmax(QK^T/d^0.5)V之后，得到out尺度仍为(bs,heads,n,1024/heads)\n",
    "4. 使用rearrange(out, 'b h n d -> b n (h d)')将heads个头的维度进行合并！\n",
    "5. 再进行MHA的Linear与Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)  # 从1024Linear成1024*3\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)  # chunk是分块 x(1,197,1024)--(1,197,3072).chunk\n",
    "        # 对tensor张量分块 x :(1, 197(196+1), 1024)   qkv 最后是一个元祖, tuple，长度是3，每个元素形状：(1, 197, 1024)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "        # q(1,16,197,64),64=1024/16;;heads=16\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale  # 效果与bmm一致，更通用。\n",
    "        # 得到了(1,16,197,197),表示的是197个块之间相互之间的关系\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 再拆FF  \n",
    "很常规的MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
